"""
Checkpoint Inspector Node - æ¨¡å‹æ£€æŸ¥å™¨èŠ‚ç‚¹
ç”¨äºåˆ†æStable Diffusionæ¨¡å‹çš„ç»“æ„å’Œç»„ä»¶
"""

import os
import torch
import safetensors.torch
import folder_paths
from .. import ModelUtils
from collections import defaultdict
import matplotlib
matplotlib.use('Agg')  # è§£å†³çº¿ç¨‹è­¦å‘Š
import matplotlib.pyplot as plt
import base64
from io import BytesIO
from PIL import Image
import io
import re
from datetime import datetime
import logging
import numpy as np
import traceback
import math
import matplotlib.gridspec as gridspec

# æ·»åŠ ä¸­æ–‡å­—ä½“é…ç½®
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei']  # ä¼˜å…ˆä½¿ç”¨ç³»ç»Ÿå­—ä½“
plt.rcParams['axes.unicode_minus'] = False

logger = logging.getLogger(__name__)

class CheckpointInspector:
    """æä¾›å®Œæ•´çš„æ¨¡å‹åˆ†æåŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
    - æ¨¡å‹æ¶æ„æ£€æµ‹ (SD1.x/SD2.x/SDXL)
    - å‚æ•°ç»Ÿè®¡å’Œç²¾åº¦åˆ†æ
    - å¯è§†åŒ–æŠ¥å‘Šç”Ÿæˆ
    - ç»„ä»¶ç»“æ„åˆ†æ
    """
    CATEGORY = "model_toolkit/åˆ†æ"
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "æ¨¡å‹": (folder_paths.get_filename_list("checkpoints"),),
                "æ˜¾ç¤ºç»†èŠ‚": (["åŸºç¡€", "é«˜çº§"], {"default": "åŸºç¡€"}),
                "æ˜¾ç¤ºå›¾è¡¨": ("BOOLEAN", {"default": True})
            }
        }
    
    def __init__(self):
        self.components = self.load_components()
        self.last_analyzed_model = None  # æ·»åŠ æ¨¡å‹ç¼“å­˜
        self.cached_result = None
    
    RETURN_TYPES = ("STRING", "IMAGE")
    RETURN_NAMES = ("åˆ†ææŠ¥å‘Š", "å¯è§†åŒ–å›¾è¡¨")
    FUNCTION = "analyze_model"
    
    def load_components(self):
        """åŠ è½½ç»„ä»¶å®šä¹‰æ–‡ä»¶"""
        components = {}
        components_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), "components")
        
        # æ‰©å±•æ”¯æŒçš„ç»„ä»¶ç±»å‹
        component_types = {
            'UNET': [
                'UNET-v1-SD.txt', 'UNET-v2-SD.txt', 'UNET-XL-SD.txt',
                'UNET-v1-EMA.txt', 'UNET-Flux-SD.txt'
            ],
            'VAE': ['VAE-v1-SD.txt'],
            'CLIP': [
                'CLIP-v1-SD.txt', 'CLIP-v2-SD.txt', 'CLIP-XL-SD.txt',
                'CLIP-v2-WD.txt'
            ],
            'EMA': ['UNET-v1-EMA.txt', 'UNET-v1-Pix2Pix-EMA.txt'] # æ·»åŠ EMAç»„ä»¶
        }
        
        for comp_type, files in component_types.items():
            components[comp_type] = {}
            for filename in files:
                try:
                    filepath = os.path.join(components_path, filename)
                    if os.path.exists(filepath):
                        with open(filepath, 'r', encoding='utf-8') as f:
                            keys = [line.strip() for line in f if line.strip() and not line.startswith('#')]
                            model_type = filename.split('-')[1]  # æå–v1/v2/XLç­‰æ ‡è¯†
                            components[comp_type][model_type] = set(keys)
                except Exception as e:
                    print(f"åŠ è½½ç»„ä»¶æ–‡ä»¶{filename}æ—¶å‡ºé”™: {str(e)}")
                    
        return components
    
    def analyze_model(self, æ¨¡å‹, æ˜¾ç¤ºç»†èŠ‚, æ˜¾ç¤ºå›¾è¡¨):
        """åˆ†ææ¨¡å‹ç»“æ„(ä¸»æ–¹æ³•)"""
        try:
            current_state = (æ¨¡å‹, æ˜¾ç¤ºç»†èŠ‚, æ˜¾ç¤ºå›¾è¡¨)
            if self.last_analyzed_model == current_state and self.cached_result:
                return self.cached_result
            self.last_analyzed_model = current_state
            
            def get_tensor_precision(tensor):
                """ç¡®å®šå¼ é‡çš„ç²¾åº¦ç±»å‹"""
                dtype = tensor.dtype
                if dtype == torch.float16:
                    return "FP16 (åŠç²¾åº¦)"
                elif dtype == torch.bfloat16:
                    return "BF16 (Brainæµ®ç‚¹)"
                elif dtype == torch.float32:
                    return "FP32 (å•ç²¾åº¦)"
                elif dtype == torch.float64:
                    return "FP64 (åŒç²¾åº¦)"
                elif dtype == torch.int8:
                    return "INT8 (é‡åŒ–)"
                elif dtype == torch.uint8:
                    return "UINT8"
                else:
                    return f"{dtype}"
            
            model_path = folder_paths.get_full_path("checkpoints", æ¨¡å‹)
            model_data = ModelUtils.load_model(model_path)
            
            if not model_data:
                return ("âŒ æ— æ³•åŠ è½½æ¨¡å‹æ–‡ä»¶", torch.zeros((1, 1, 3)))

            result = []
            result.append(f"ğŸ“„ æ¨¡å‹æ–‡ä»¶: {æ¨¡å‹}")
            result.append(f"ğŸ“Š æ¨¡å‹å¤§å°: {ModelUtils.format_size(os.path.getsize(model_path))}")
            total_params = sum(t.numel() for t in model_data.values())
            
            precision_stats = self.analyze_model_precision(model_data)
            
            result.append(f"ğŸ”¢ å‚æ•°æ€»é‡: {self.format_params(total_params)} [{precision_stats['primary_precision']}]")
            
            arch_result = self.analyze_architecture(model_data)
            if isinstance(arch_result, tuple):
                arch_info = arch_result[0]
            else:
                arch_info = arch_result
            
            result.append(f"\nğŸ—ï¸ æ¨¡å‹æ¶æ„: {arch_info['type']}")
            if arch_info.get('extra_info'):
                for info in arch_info['extra_info']:
                    result.append(f"  â€¢ {info}")
            
            groups = ModelUtils.group_model_keys(model_data)
            
            comp_params = defaultdict(lambda: (0, 0.0))
            for comp_name, keys in groups.items():
                params = sum(model_data[k].numel() for k in keys if k in model_data)
                comp_params[comp_name] = (params, params / total_params * 100)
            
            for k in ['UNET', 'CLIP', 'VAE']:
                if k not in comp_params:
                    comp_params[k] = (0, 0.0)
            
            # è®¡ç®—å…¶ä»–å‚æ•°
            other_params = comp_params.get('å…¶ä»–', (0, 0.0))[0]
            if other_params > 0:
                other_percent = other_params / total_params * 100
                comp_params['å…¶ä»–'] = (other_params, other_percent)
            
            # è®¡ç®—EMAå‚æ•°
            ema_keys = []
            
            # æ–¹æ³•1: æ‰€æœ‰å¯èƒ½çš„EMAå‘½åæ¨¡å¼
            ema_patterns = ['model_ema', 'ema.', '.ema', '_ema', 'ema_', 'EMA']
            for pattern in ema_patterns:
                ema_keys.extend([k for k in model_data.keys() if pattern in k])
            
            # æ–¹æ³•2: æ£€æŸ¥componentsç›®å½•ä¸­çš„æ‰€æœ‰EMAæ–‡ä»¶
            components_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), "components")
            try:
                for filename in os.listdir(components_dir):
                    if 'EMA' in filename and filename.endswith('.txt'):
                        ema_file_path = os.path.join(components_dir, filename)
                        with open(ema_file_path, 'r', encoding='utf-8') as f:
                            for line in f:
                                line = line.strip()
                                if line and not line.startswith('#'):
                                    # åŒ¹é…æ‰€æœ‰åŒ…å«æ­¤æ¨¡å¼çš„é”®
                                    matching_keys = [k for k in model_data.keys() if line in k]
                                    ema_keys.extend(matching_keys)
            except Exception as e:
                logger.error(f"åŠ è½½EMAç»„ä»¶æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
            
            # ç§»é™¤é‡å¤é¡¹
            ema_keys = list(set(ema_keys))
            ema_params = sum(model_data[k].numel() for k in ema_keys if k in model_data)
            
            # è°ƒæ•´UNETå‚æ•°ï¼Œæ’é™¤EMAéƒ¨åˆ†
            if ema_params > 0:
                # è®¡ç®—é‡å éƒ¨åˆ†ï¼šåŒæ—¶å±äºUNETå’ŒEMAçš„å‚æ•°
                unet_keys = groups.get('UNET', [])
                overlap_keys = [k for k in ema_keys if k in unet_keys]
                overlap_params = sum(model_data[k].numel() for k in overlap_keys if k in model_data)
                
                # ä»UNETä¸­å‡å»é‡å éƒ¨åˆ†
                if 'UNET' in comp_params:
                    unet_params, unet_percent = comp_params['UNET']
                    adj_unet_params = unet_params - overlap_params
                    adj_unet_percent = adj_unet_params / total_params * 100
                    comp_params['UNET'] = (adj_unet_params, adj_unet_percent)
                
                # æ·»åŠ EMAä½œä¸ºç‹¬ç«‹ç»„ä»¶
                comp_params['EMA'] = (ema_params, ema_params / total_params * 100)
                
                # æ›´æ–°æ–‡æœ¬æŠ¥å‘Š
                result.append(f"\nâš¡ EMAå‚æ•°:")
                result.append(f"- EMAå‚æ•°é‡: {self.format_params(ema_params)} ({ema_params/total_params*100:.1f}%)")
                result.append(f"- EMAé”®æ•°é‡: {len(ema_keys)}")
            
            # è·å–æœªå½’ç±»é”®å
            classified_keys = set()
            for group_name, keys in groups.items():
                classified_keys.update(keys)
            
            unknown_keys = [k for k in model_data.keys() if k not in classified_keys and k not in ema_keys]
            unknown_params = sum(model_data[k].numel() for k in unknown_keys if k in model_data)
            
            # å¦‚æœæœ‰æœªå½’ç±»å‚æ•°ï¼Œæ·»åŠ åˆ°ç»“æœ
            if unknown_params > 0:
                comp_params['æœªçŸ¥'] = (unknown_params, unknown_params/total_params*100)
                result.append(f"\nğŸ” æœªå½’ç±»å‚æ•°:")
                result.append(f"- æœªå½’ç±»å‚æ•°é‡: {self.format_params(unknown_params)} ({unknown_params/total_params*100:.1f}%)")
                result.append(f"- æœªå½’ç±»é”®æ•°é‡: {len(unknown_keys)}")
            
            # å¦‚æœæœ‰å…¶ä»–å‚æ•°ï¼Œæ·»åŠ åˆ°ç»“æœ
            if other_params > 0:
                result.append(f"\nğŸ“¦ å…¶ä»–å‚æ•°:")
                result.append(f"- å…¶ä»–å‚æ•°é‡: {self.format_params(other_params)} ({other_percent:.1f}%)")
                result.append(f"- å…¶ä»–é”®æ•°é‡: {len(groups.get('å…¶ä»–', []))}")
            
            # å…ƒæ•°æ®æå–
            metadata = {}
            for k in model_data.keys():
                if k.startswith('_metadata') or '_metadata' in k:
                    metadata[k] = model_data[k]
            
            if metadata:
                result.append(f"\nğŸ“Œ æ¨¡å‹å…ƒä¿¡æ¯:")
                for key, value in metadata.items():
                    if hasattr(value, 'tolist'):
                        try:
                            value = value.tolist()
                        except:
                            value = str(value)
                    result.append(f"- {key}: {value}")
            else:
                result.append(f"\nğŸ“Œ æ¨¡å‹æœªåŒ…å«å…ƒä¿¡æ¯")

            # è¯¦ç»†åˆ†æï¼ˆåŸºç¡€/é«˜çº§æ¨¡å¼ï¼‰
            if æ˜¾ç¤ºç»†èŠ‚ == "åŸºç¡€":
                # UNETåˆ†æ
                if 'UNET' in groups and groups['UNET']:
                    unet_results = self.analyze_unet_structure(model_data, groups['UNET'], total_params)
                    result.extend(unet_results)
                
                # VAEåˆ†æ
                if 'VAE' in groups and groups['VAE']:
                    vae_results = self.analyze_vae_structure(model_data, groups['VAE'], total_params)
                    result.extend(vae_results)
                
                # CLIPåˆ†æ
                if 'CLIP' in groups and groups['CLIP']:
                    clip_results = self.analyze_clip_structure(model_data, groups['CLIP'], total_params, arch_info['type'])
                    result.extend(clip_results)

            elif æ˜¾ç¤ºç»†èŠ‚ == "é«˜çº§":
                result.append("\nğŸ” æ¨¡å‹æ‰€æœ‰é”®å:")
                all_keys = set(model_data.keys())
                
                for comp_name, keys in groups.items():
                    if keys:
                        result.append(f"\n=== {comp_name} é”®å ===")
                        for key in sorted(keys):
                            if key in model_data:
                                result.append(f"  {key}")
                                all_keys.discard(key)

                if all_keys:
                    result.append("\n=== æœªåˆ†ç±»é”®å ===")
                    for key in sorted(all_keys):
                        result.append(f"  {key}")

            # åˆ›å»ºåˆ†ææ•°æ®å­—å…¸
            analysis_data = {
                'model_name': æ¨¡å‹,
                'model_size': ModelUtils.format_size(os.path.getsize(model_path)),
                'total_params': total_params,
                'total_params_formatted': self.format_params(total_params),
                'primary_precision': precision_stats['primary_precision'],
                'precision_distributions': precision_stats.get('distributions', {}),
                'components': {}, 
                'architecture': arch_info
            }
            
            # å¡«å……ç»„ä»¶æ•°æ®
            for comp_name, (params, percentage) in comp_params.items():
                analysis_data['components'][comp_name] = params
            
            # UNETç»†åˆ†å›¾æ•°æ®è®¡ç®—
            unet_parts = {}
            if 'UNET' in groups and groups['UNET']:
                # è®¡ç®—å„å—çš„å‚æ•°é‡å æ¯” - æ’é™¤EMAå‚æ•°
                input_keys = [k for k in groups['UNET'] if any(p in k for p in ['input_blocks', 'down_blocks']) and k not in ema_keys]
                middle_keys = [k for k in groups['UNET'] if any(p in k for p in ['middle_block', 'mid_block', 'bottleneck']) and k not in ema_keys]
                output_keys = [k for k in groups['UNET'] if any(p in k for p in ['output_blocks', 'up_blocks']) and k not in ema_keys]
                
                input_params = sum(model_data[k].numel() for k in input_keys if k in model_data)
                middle_params = sum(model_data[k].numel() for k in middle_keys if k in model_data)
                output_params = sum(model_data[k].numel() for k in output_keys if k in model_data)
                
                total_unet_params = input_params + middle_params + output_params
                if total_unet_params > 0:
                    unet_parts = {
                        'è¾“å…¥å—': input_params / total_unet_params,
                        'ä¸­é—´å—': middle_params / total_unet_params,
                        'è¾“å‡ºå—': output_params / total_unet_params
                    }
                    analysis_data['unet_parts'] = unet_parts
            
            # ç”Ÿæˆå›¾è¡¨
            image_tensor = torch.zeros((1, 1, 3))
            if æ˜¾ç¤ºå›¾è¡¨:
                try:
                    chart_data = self.generate_visual_report(analysis_data, æ¨¡å‹, arch_info['type'])
                    if chart_data:
                        image_tensor = self.convert_image(chart_data)
                except Exception as e:
                    logger.error(f"ç”Ÿæˆå›¾è¡¨å¤±è´¥: {str(e)}")
            
            self.cached_result = ("\n".join(result), image_tensor)
            return self.cached_result
            
        except Exception as e:
            logger.error(f"åˆ†æå¤±è´¥: {str(e)}")
            return (f"âŒ åˆ†æå¤±è´¥: {str(e)}", torch.zeros((1, 1, 3)))

    def analyze_architecture(self, model_data):
        """æ”¯æŒæ£€æµ‹ä»¥ä¸‹æ¶æ„ï¼š
        - SD1.x (å«EMAå˜ä½“)
        - SD2.x (é€šè¿‡v_predæ£€æµ‹)
        - SDXL (é€šè¿‡åŒCLIPæ£€æµ‹)
        """
        result = {'type': 'æœªçŸ¥', 'extra_info': []}
        total_params = sum(t.numel() for t in model_data.values())

        # è°ƒè¯•æ ‡è®° - æ”¶é›†å…³é”®ä¿¡æ¯
        debug_info = {}
        
        # 1. æ”¶é›†å…³é”®ç»“æ„ä¿¡æ¯
        # ====================
        
        # æ£€æµ‹è¾“å…¥/è¾“å‡ºå—æ•°é‡
        block_indices = set()
        for k in [key for key in model_data.keys() if 'input_blocks' in key or 'down_blocks' in key]:
            try:
                parts = k.split('.')
                for i, part in enumerate(parts):
                    if part in ['input_blocks', 'down_blocks'] and i+1 < len(parts):
                        if parts[i+1].isdigit():
                            block_idx = int(parts[i+1])
                            block_indices.add(block_idx)
            except Exception as e:
                print(f"å—ç´¢å¼•è§£æé”™è¯¯: {e}")
        
        block_count = len(block_indices)
        max_block_idx = max(block_indices) if block_indices else -1
        debug_info['block_count'] = block_count
        debug_info['max_block_idx'] = max_block_idx
        
        # 2. UNETè¾“å…¥é€šé“æ£€æµ‹ç­–ç•¥ 
        # =======================
        
        # å°è¯•å¤šç§å¯èƒ½çš„è¾“å…¥å±‚ä½ç½®
        unet_input_patterns = [
            # SDXLæ ¼å¼
            'model.diffusion_model.input_blocks.0.0.weight',
            'diffusion_model.input_blocks.0.0.weight',
            'model.diffusion_model.input_blocks.0.0.conv.weight',
            'diffusion_model.input_blocks.0.0.conv.weight',
            # SDæ ¼å¼
            'model.model.diffusion_model.input_blocks.0.0.weight',
            'model.model.diffusion_model.input_blocks.0.0.conv.weight',
            # ç¨³å®šæ€§æ”¹è¿›æ ¼å¼
            'model.input_blocks.0.0.weight',
            'model.down_blocks.0.resnets.0.conv1.weight',
            # æœ€åå°è¯•æ›´é€šç”¨çš„æ¨¡å¼
            'input_blocks.0',
            'down_blocks.0'
        ]
        
        # æœç´¢å¯èƒ½çš„è¾“å…¥å±‚
        found_input_keys = []
        for pattern in unet_input_patterns:
            matching_keys = [k for k in model_data.keys() if pattern in k]
            if matching_keys:
                found_input_keys.extend(matching_keys)
        
        # æå–å®é™…é€šé“æ•°
        input_channels = None
        channel_key = None
        
        for key in found_input_keys:
            if key in model_data:
                tensor = model_data[key]
                # æ£€æŸ¥æ˜¯å¦ä¸ºå·ç§¯æƒé‡ 
                if len(tensor.shape) == 4:
                    # å·ç§¯æƒé‡å½¢çŠ¶: [out_channels, in_channels, kernel_h, kernel_w]
                    input_channels = tensor.shape[1]  # ç¬¬äºŒç»´æ˜¯è¾“å…¥é€šé“
                    channel_key = key
                    break
        
        debug_info['input_channel_detection'] = {
            'found_keys': len(found_input_keys),
            'channel_key': channel_key,
            'input_channels': input_channels
        }
        
        # 3. CLIPæ¨¡å‹æ£€æµ‹
        # ==============
        
        # æ£€æµ‹text_model_1
        has_clip_l = any(
            k for k in model_data.keys() 
            if 'text_model.encoder' in k or 'cond_stage_model.transformer' in k
        )
        
        # æ£€æµ‹text_model_2 (SDXLç‰¹æœ‰)
        has_clip_g = any(
            k for k in model_data.keys() 
            if 'text_model_2' in k or 'conditioner.embedders.1' in k
        )
        
        debug_info['clip_detection'] = {
            'has_clip_l': has_clip_l,
            'has_clip_g': has_clip_g
        }
        
        # 4. æ¶æ„ç‰¹å¾åˆ†æ
        # =============
        
        # SD1.xç‰¹å¾
        sd1_features = {
            'block_count': block_count <= 12 and block_count >= 9,
            'input_channels': input_channels == 4,
            'param_count': total_params < 2e9,
            'single_clip': has_clip_l and not has_clip_g
        }
        
        # SD2.xç‰¹å¾
        sd2_features = {
            'block_count': block_count <= 12 and block_count >= 9,
            'input_channels': input_channels == 4,
            'param_count': total_params < 2e9, 
            'v_pred': any('v_pred' in k for k in model_data.keys()),
            'single_clip': has_clip_l and not has_clip_g
        }
        
        # SDXLç‰¹å¾
        sdxl_features = {
            'block_count': block_count >= 9,
            'dual_clip': has_clip_l and has_clip_g,
            'param_count': total_params > 2.5e9
        }
        
        debug_info['architecture_scores'] = {
            'sd1': sum(1 for v in sd1_features.values() if v),
            'sd2': sum(1 for v in sd2_features.values() if v),
            'sdxl': sum(1 for v in sdxl_features.values() if v)
        }
        
        # 5. æ¶æ„åˆ¤å®š
        # ==========
        
        # è®°å½•å†³ç­–æ—¥å¿—
        decision_log = []
        
        # SDXLåˆ¤å®šï¼ˆä¿®æ”¹åï¼‰
        if sdxl_features['dual_clip'] and (sdxl_features['block_count'] or sdxl_features['param_count']):
            result['type'] = "SDXL"
            decision_log.append("å‘ç°åŒCLIPç»“æ„ï¼ŒSDXLç‰¹å¾æ˜æ˜¾")
            
            # æ£€æŸ¥UNETè¾“å…¥é€šé“
            if input_channels == 9:
                result['extra_info'].append(f"UNETè¾“å…¥é€šé“æ•°: {input_channels} (6ä¸ªæ¡ä»¶é€šé“ + 1ä¸ªå™ªå£°é€šé“ + 2ä¸ªåˆ†è¾¨ç‡é€šé“)")
            elif input_channels == 4:
                result['extra_info'].append(f"UNETè¾“å…¥é€šé“æ•°: {input_channels} (3ä¸ªæ¡ä»¶é€šé“ + 1ä¸ªå™ªå£°é€šé“) [æ³¨æ„: ä¸æ ‡å‡†SDXLçš„9é€šé“ä¸ç¬¦]")
                decision_log.append("è¾“å…¥é€šé“æ•°(4)å¼‚å¸¸ï¼Œå¯èƒ½æ˜¯ç‰¹æ®Šè½¬æ¢ç‰ˆæœ¬")
            else:
                result['extra_info'].append(f"UNETè¾“å…¥é€šé“æ•°: {input_channels} [éæ ‡å‡†é…ç½®]")
            
            # ä¿®æ”¹ï¼šä¸å†æ˜¾ç¤ºEMAå‚æ•°ï¼Œæ”¹ä¸ºæ˜¾ç¤ºCLIPå’ŒVAEçš„é€šé“ä¿¡æ¯
            result['extra_info'].append("CLIPæ–‡æœ¬è¾“å…¥ç»´åº¦: 768 (æœ€å¤§é•¿åº¦: 77)")
            result['extra_info'].append("VAEè¾“å…¥é€šé“æ•°: 3 (RGBå›¾åƒè¾“å…¥)")
            
            if channel_key:
                decision_log.append(f"æ£€æµ‹åˆ°çš„è¾“å…¥å±‚: {channel_key}")
            
            return result, debug_info
        
        # SD2.xåˆ¤å®š
        if sd2_features['v_pred']:
            result['type'] = "SD2.x"
            decision_log.append("å‘ç°v_predé¢„æµ‹å™¨ï¼Œè¯†åˆ«ä¸ºSD2.x")
            result['extra_info'].append(f"UNETè¾“å…¥é€šé“æ•°: {input_channels} (3ä¸ªæ¡ä»¶é€šé“ + 1ä¸ªå™ªå£°é€šé“)")
            result['extra_info'].append("æ£€æµ‹åˆ°v_predé¢„æµ‹å™¨")
            return result, debug_info
        
        # SD1.xåˆ¤å®šï¼ˆä¿®æ”¹åï¼‰
        if sd1_features['block_count'] and sd1_features['input_channels']:
            # ä¸å†è¾“å‡ºEMAæ•°é‡ï¼Œè€Œæ˜¯ç›´æ¥æ˜¾ç¤ºUNETã€VAEå’ŒCLIPçš„é€šé“ä¿¡æ¯
            result['type'] = "SD1.x-EMA" if len([k for k in model_data.keys() if 'model_ema' in k]) > 0 else "SD1.x"
            result['extra_info'].append(f"UNETè¾“å…¥é€šé“æ•°: {input_channels} (3ä¸ªæ¡ä»¶é€šé“ + 1ä¸ªå™ªå£°é€šé“)")
            result['extra_info'].append("VAEè¾“å…¥é€šé“æ•°: 3 (RGBå›¾åƒè¾“å…¥)")
            result['extra_info'].append("CLIPæ–‡æœ¬è¾“å…¥ç»´åº¦: 768 (æœ€å¤§é•¿åº¦: 77)")
            return result, debug_info
        
        # æœªèƒ½ç¡®å®šç±»å‹
        result['type'] = "æœªçŸ¥"
        result['extra_info'].append(f"æ— æ³•ç¡®å®šæ¶æ„ç±»å‹")
        if input_channels:
            result['extra_info'].append(f"UNETè¾“å…¥é€šé“æ•°: {input_channels}")
        
        return result, debug_info

    def format_params(self, num):
        """æ ¼å¼åŒ–å‚æ•°æ•°é‡ï¼Œ1Bä»¥ä¸Šç”¨Bå•ä½ï¼Œå¦åˆ™ç”¨Må•ä½"""
        if num >= 1e9:
            return f"{num/1e9:.2f}B"
        else:
            return f"{num/1e6:.2f}M"

    def analyze_unet_structure(self, model_data, unet_keys, total_params):
        """åˆ†æUNETç»“æ„"""
        result = []
        result.append("\nUNETç»“æ„:")
        
        key_groups = {
            'è¾“å…¥å—': [k for k in unet_keys if any(p in k for p in ['input_blocks', 'down_blocks'])],
            'ä¸­é—´å—': [k for k in unet_keys if any(p in k for p in ['middle_block', 'mid_block', 'bottleneck'])],
            'è¾“å‡ºå—': [k for k in unet_keys if any(p in k for p in ['output_blocks', 'up_blocks'])]
        }
        
        def extract_indices(keys, pattern):
            """æå–ç‰¹å®šæ¨¡å¼çš„é”®ç´¢å¼•"""
            indices = set()
            for k in keys:
                if pattern in k:
                    parts = k.split(pattern)
                    if len(parts) > 1:
                        next_part = parts[1].lstrip('.')
                        if '.' in next_part:
                            index_str = next_part.split('.')[0]
                            if index_str.isdigit():
                                indices.add(int(index_str))
            return sorted(list(indices))
        
        total_unet_params = 0
        for group_name, keys in key_groups.items():
            params = sum(model_data[k].numel() for k in keys if k in model_data)
            percentage = params / total_params * 100
            total_unet_params += params
            
            if group_name == 'è¾“å…¥å—':
                indices = extract_indices(keys, 'input_blocks')
                if not indices:
                    indices = extract_indices(keys, 'down_blocks')
            elif group_name == 'è¾“å‡ºå—':
                indices = extract_indices(keys, 'output_blocks')
                if not indices:
                    indices = extract_indices(keys, 'up_blocks')
            else:
                indices = []
            
            if indices:
                max_idx = max(indices)
                result.append(f"- {group_name}: {max_idx + 1} (ç´¢å¼•: 0-{max_idx})")
            else:
                count = 1 if group_name == 'ä¸­é—´å—' else 0
                result.append(f"- {group_name}: {count}")
            
            result.append(f"  å‚æ•°é‡: {self.format_params(params)} ({percentage:.1f}%)")
        
        result.append(f"- UNETæ€»å‚æ•°é‡: {self.format_params(total_unet_params)} ({total_unet_params/total_params*100:.1f}%)")
        
        return result

    def analyze_vae_structure(self, model_data, vae_keys, total_params):
        """åˆ†æVAEç»“æ„"""
        result = []
        result.append("\nVAEç»“æ„:")
        
        encoders = [k for k in vae_keys if 'encoder' in k]
        decoders = [k for k in vae_keys if 'decoder' in k]
        
        encoder_params = sum(model_data[k].numel() for k in encoders if k in model_data)
        decoder_params = sum(model_data[k].numel() for k in decoders if k in model_data)
        total_vae_params = encoder_params + decoder_params
        
        enc_percent = encoder_params / total_params * 100
        dec_percent = decoder_params / total_params * 100
        vae_percent = total_vae_params / total_params * 100
        
        vae_input = next((k for k in vae_keys if 'encoder.conv_in.weight' in k), None)
        if vae_input and vae_input in model_data:
            shape = model_data[vae_input].shape
            in_channels = shape[1] if len(shape) >= 2 else 0
            result.append(f"- è¾“å…¥é€šé“æ•°: {in_channels}")
        
        result.append(f"- ç¼–ç å™¨é”®æ•°é‡: {len(encoders)}")
        result.append(f"- ç¼–ç å™¨å‚æ•°é‡: {self.format_params(encoder_params)} ({enc_percent:.1f}%)")
        result.append(f"- è§£ç å™¨é”®æ•°é‡: {len(decoders)}")
        result.append(f"- è§£ç å™¨å‚æ•°é‡: {self.format_params(decoder_params)} ({dec_percent:.1f}%)")
        result.append(f"- VAEæ€»å‚æ•°é‡: {self.format_params(total_vae_params)} ({vae_percent:.1f}%)")
        
        return result

    def analyze_clip_structure(self, model_data, clip_keys, total_params, arch_type):
        """åˆ†æCLIPç»“æ„"""
        result = []
        result.append("\nCLIPç»“æ„:")
        
        # æ›´åŠ ç²¾ç¡®åœ°è¯†åˆ«text_model_1å’Œtext_model_2
        text_model_patterns = {
            'text_model_1': [
                'text_model.encoder', 
                'text_model.embeddings',
                'conditioner.embedders.0',
                'cond_stage_model'
            ],
            'text_model_2': [
                'text_model_2',
                'conditioner.embedders.1'
            ]
        }
        
        text_model_1_keys = []
        text_model_2_keys = []
        
        # æ”¶é›†text_model_1å’Œtext_model_2çš„é”®
        for k in clip_keys:
            if any(pattern in k for pattern in text_model_patterns['text_model_1']):
                text_model_1_keys.append(k)
            elif any(pattern in k for pattern in text_model_patterns['text_model_2']):
                text_model_2_keys.append(k)
        
        # ç¡®å®šæ˜¯å¦æœ‰text_model_2æˆ–æ˜¯SDXLæ¨¡å‹ç±»å‹
        has_text_model_2 = len(text_model_2_keys) > 0 or arch_type == "SDXL"
        
        # æ·»åŠ text_model_1ä¿¡æ¯
        if text_model_1_keys:
            model_params = sum(model_data[k].numel() for k in text_model_1_keys if k in model_data)
            model_percent = model_params / total_params * 100
            result.append(f"- text_model_1é”®æ•°é‡: {len(text_model_1_keys)}")
            result.append(f"- text_model_1å‚æ•°é‡: {self.format_params(model_params)} ({model_percent:.1f}%)")
        
        # æ·»åŠ text_model_2ä¿¡æ¯ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if text_model_2_keys:
            model_params = sum(model_data[k].numel() for k in text_model_2_keys if k in model_data)
            model_percent = model_params / total_params * 100
            result.append(f"- text_model_2é”®æ•°é‡: {len(text_model_2_keys)}")
            result.append(f"- text_model_2å‚æ•°é‡: {self.format_params(model_params)} ({model_percent:.1f}%)")
        # å¯¹äºSDXLç±»å‹ï¼Œä½†æ²¡æ£€æµ‹åˆ°text_model_2é”®çš„æƒ…å†µ
        elif arch_type == "SDXL" and not text_model_2_keys:
            result.append(f"- text_model_2: æœªæ£€æµ‹åˆ° (å¯èƒ½æ˜¯SDXLå˜ç§)")
        
        # æ€»CLIPå‚æ•°é‡
        total_clip_params = sum(model_data[k].numel() for k in clip_keys if k in model_data)
        total_clip_percent = total_clip_params / total_params * 100
        result.append(f"- CLIPæ€»å‚æ•°é‡: {self.format_params(total_clip_params)} ({total_clip_percent:.1f}%)")
        
        return result

    def analyze_model_precision(self, model_data):
        """æ”¯æŒæ£€æµ‹ï¼š
        - FP16/FP32/BF16
        - INT8/INT4é‡åŒ–
        - æ··åˆç²¾åº¦æ¨¡å‹
        """
        precision_counts = defaultdict(int)
        total_params = 0
        
        # ç²¾åº¦ç±»å‹æ˜ å°„è¡¨
        precision_map = {
            'torch.float32': 'FP32 (å•ç²¾åº¦)',
            'torch.float16': 'FP16 (åŠç²¾åº¦)',
            'torch.bfloat16': 'BF16 (è„‘æµ®ç‚¹)',
            'torch.float8_e4m3fn': 'FP8 (E4M3)',
            'torch.float8_e5m2': 'FP8 (E5M2)',
            'torch.int8': 'INT8 (é‡åŒ–)',
            'torch.int4': 'INT4 (é‡åŒ–)',
            'torch.uint8': 'UINT8 (é‡åŒ–)',
        }
        
        # ç²¾åº¦å‹å¥½åç§°
        precision_friendly = {
            'float32': 'FP32 (å•ç²¾åº¦)',
            'float16': 'FP16 (åŠç²¾åº¦)',
            'bfloat16': 'BF16 (è„‘æµ®ç‚¹)',
            'float8': 'FP8',
            'int8': 'INT8 (é‡åŒ–)',
            'int4': 'INT4 (é‡åŒ–)',
            'uint8': 'UINT8 (é‡åŒ–)',
        }
        
        # éå†æ‰€æœ‰å¼ é‡ç»Ÿè®¡ç²¾åº¦åˆ†å¸ƒ
        for key, tensor in model_data.items():
            param_count = tensor.numel()
            total_params += param_count
            
            # è·å–æ•°æ®ç±»å‹å¹¶æ˜ å°„åˆ°å‹å¥½åç§°
            dtype_str = str(tensor.dtype)
            
            # æå–æ•°æ®ç±»å‹çš„æ ¸å¿ƒéƒ¨åˆ† (ä¾‹å¦‚ä»"torch.float32"æå–"float32")
            if 'torch.' in dtype_str:
                dtype_core = dtype_str.split('torch.')[1]
            else:
                dtype_core = dtype_str
            
            # æŸ¥æ‰¾å‹å¥½åç§°ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨åŸå§‹ç±»å‹å
            if dtype_core in precision_friendly:
                precision_type = precision_friendly[dtype_core]
            elif dtype_str in precision_map:
                precision_type = precision_map[dtype_str]
            else:
                precision_type = f"å…¶ä»– ({dtype_str})"
            
            precision_counts[precision_type] += param_count
        
        # è®¡ç®—ç™¾åˆ†æ¯”å¹¶ç¡®å®šä¸»è¦ç²¾åº¦ç±»å‹
        precision_distributions = {}
        primary_precision = "æ··åˆç²¾åº¦"
        max_count = 0
        
        for prec_type, count in precision_counts.items():
            percentage = (count / total_params) * 100
            precision_distributions[prec_type] = (count, percentage)
            
            # æ›´æ–°ä¸»è¦ç²¾åº¦ç±»å‹(è¶…è¿‡90%çš„å‚æ•°ä½¿ç”¨åŒä¸€ç²¾åº¦)
            if count > max_count and percentage > 90:
                max_count = count
                primary_precision = prec_type
        
        # å¦‚æœæ²¡æœ‰æ˜ç¡®ä¸»è¦ç²¾åº¦ï¼Œæ£€æŸ¥æ˜¯å¦ä¸ºæ··åˆç²¾åº¦è®­ç»ƒå…¸å‹ç»„åˆ
        if primary_precision == "æ··åˆç²¾åº¦":
            fp16_percentage = precision_distributions.get('FP16 (åŠç²¾åº¦)', (0, 0))[1]
            fp32_percentage = precision_distributions.get('FP32 (å•ç²¾åº¦)', (0, 0))[1]
            
            if fp16_percentage > 50 and fp32_percentage > 0:
                primary_precision = "æ··åˆç²¾åº¦ (ä¸»è¦FP16)"
            elif 'INT8 (é‡åŒ–)' in precision_distributions and precision_distributions['INT8 (é‡åŒ–)'][1] > 50:
                primary_precision = "é‡åŒ–æ¨¡å‹ (INT8)"
        
        return {
            'distributions': precision_distributions,
            'primary_precision': primary_precision,
            'total_params': total_params
        }

    def analyze_component_precision(self, model_data, groups):
        """åˆ†ææ¯ä¸ªç»„ä»¶çš„ç²¾åº¦åˆ†å¸ƒ"""
        precision_stats = {}
        
        # ç²¾åº¦å‹å¥½åç§°
        precision_friendly = {
            'float32': 'FP32 (å•ç²¾åº¦)',
            'float16': 'FP16 (åŠç²¾åº¦)',
            'bfloat16': 'BF16 (è„‘æµ®ç‚¹)',
            'float8': 'FP8',
            'int8': 'INT8 (é‡åŒ–)',
            'int4': 'INT4 (é‡åŒ–)',
            'uint8': 'UINT8 (é‡åŒ–)'
        }
        
        # å¤„ç†æ¯ä¸ªç»„ä»¶
        for comp_name, keys in groups.items():
            comp_precision = defaultdict(lambda: (0, 0.0))
            comp_total = 0
            
            # åˆ†æç»„ä»¶å†…æ¯ä¸ªå¼ é‡çš„ç²¾åº¦
            for key in keys:
                if key in model_data:
                    tensor = model_data[key]
                    param_count = tensor.numel()
                    comp_total += param_count
                    
                    # è·å–ç²¾åº¦ç±»å‹åç§°
                    dtype_str = str(tensor.dtype)
                    if 'torch.' in dtype_str:
                        dtype_core = dtype_str.split('torch.')[1]
                    else:
                        dtype_core = dtype_str
                    
                    # æŸ¥æ‰¾å‹å¥½åç§°
                    if dtype_core in precision_friendly:
                        precision_type = precision_friendly[dtype_core]
                    else:
                        precision_type = f"å…¶ä»– ({dtype_str})"
                    
                    # æ›´æ–°è®¡æ•°
                    curr_count, curr_pct = comp_precision[precision_type]
                    comp_precision[precision_type] = (curr_count + param_count, 0)  # ç™¾åˆ†æ¯”ç¨åè®¡ç®—
            
            # è®¡ç®—ç™¾åˆ†æ¯”
            if comp_total > 0:
                for prec_type in comp_precision:
                    count, _ = comp_precision[prec_type]
                    comp_precision[prec_type] = (count, count / comp_total * 100)
                
                precision_stats[comp_name] = dict(comp_precision)
        
        return precision_stats

    def generate_visual_report(self, analysis_data, model_name, arch_type):
        """ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š"""
        try:
            # è®¾ç½®å›¾è¡¨é£æ ¼
            plt.style.use('dark_background')
            # ä½¿ç”¨ç¨å¤§ä¸€äº›çš„ç”»å¸ƒ
            plt.figure(figsize=(20, 12), dpi=100) 
            
            # æ·»åŠ æ ‡é¢˜
            plt.suptitle(
                f"{model_name} ç»“æ„åˆ†æ - {arch_type}\n"
                f"æ€»å‚æ•°é‡: {analysis_data['total_params_formatted']} | "
                f"ä¸»è¦ç²¾åº¦: {analysis_data['primary_precision']}",
                color='white', fontsize=20, y=0.98
            )
            
            # ä½¿ç”¨3x3ç½‘æ ¼å¸ƒå±€
            grid = plt.GridSpec(3, 3, hspace=0.4, wspace=0.3)
            
            # å·¦ä¸Šï¼šç»„ä»¶æ¯”ä¾‹åœ†ç¯å›¾
            ax_pie = plt.subplot(grid[0:2, 0])
            self.draw_component_pie(ax_pie, analysis_data)
            
            # å³ä¾§ï¼šè¯¦ç»†å‚æ•°æ¡å½¢å›¾
            ax_bars = plt.subplot(grid[:, 1:3])
            self.draw_param_bars(ax_bars, analysis_data, arch_type)
            
            # å·¦ä¸‹ï¼šUNETç»“æ„
            ax_unet = plt.subplot(grid[2, 0])
            self.draw_unet_structure(ax_unet, analysis_data)
            
            # æ·»åŠ æ°´å°
            plt.figtext(0.02, 0.01, "ç”± ComfyUI Model Toolkit ç”Ÿæˆ", color='gray', alpha=0.5)
            
            # è°ƒæ•´å¸ƒå±€
            plt.tight_layout(rect=[0, 0, 1, 0.95])
            
            # ä¿å­˜ä¸ºä¸´æ—¶å†…å­˜ä¸­çš„å›¾åƒ
            buf = io.BytesIO()
            plt.savefig(buf, format='png', facecolor='#1E1E1E')
            plt.close()
            
            # ä½¿ç”¨PILè°ƒæ•´å›¾åƒåˆ°ç²¾ç¡®å°ºå¯¸
            buf.seek(0)
            img = Image.open(buf)
            img_resized = img.resize((2048, 1280), Image.Resampling.LANCZOS)
            
            # ä¿å­˜è°ƒæ•´åçš„å›¾åƒ
            buf_resized = io.BytesIO()
            img_resized.save(buf_resized, format='PNG')
            buf_resized.seek(0)
            
            return f"data:image/png;base64,{base64.b64encode(buf_resized.getvalue()).decode()}"
        except Exception as e:
            logger.error(f"ç”Ÿæˆå›¾è¡¨å‡ºé”™: {str(e)}")
            return None

    def draw_component_pie(self, ax, data):
        """ç»˜åˆ¶ç»„ä»¶æ„æˆæ¯”ä¾‹é¥¼å›¾"""
        components = data.get('components', {})
        if not components:
            ax.text(0.5, 0.5, 'æ— ç»„ä»¶æ•°æ®', ha='center', va='center', color='white')
            return
        
        # è·å–ç»„ä»¶å‚æ•°é‡å’Œæ¯”ä¾‹ï¼Œè¿‡æ»¤æ‰æœªçŸ¥å’Œå æ¯”å¾ˆå°çš„ç»„ä»¶
        names = []
        values = []
        explode = []
        
        # åŸºç¡€ç»„ä»¶ä¼˜å…ˆæ˜¾ç¤º
        base_components = ['UNET', 'VAE', 'CLIP', 'EMA']
        for comp in base_components:
            if comp in components and components[comp] > 0:
                names.append(comp)
                values.append(components[comp])
                explode.append(0.05 if comp == 'UNET' else 0)
        
        # æ·»åŠ å…¶ä»–ç»„ä»¶
        for comp, value in components.items():
            if comp not in base_components and value > 0:
                if comp == 'æœªçŸ¥' and value/sum(components.values()) < 0.01:
                    continue  # è·³è¿‡å æ¯”æå°çš„æœªçŸ¥ç»„ä»¶
                names.append(comp)
                values.append(value)
                explode.append(0)
        
        # é¢œè‰²æ˜ å°„
        colors = {
            'UNET': '#4C72B0', 
            'VAE': '#55A868', 
            'CLIP': '#C44E52', 
            'EMA': '#DD8452',
            'æœªçŸ¥': '#937860',
            'å…¶ä»–': '#8172B0'
        }
        pie_colors = [colors.get(name, '#cccccc') for name in names]
        
        # ä½¿ç”¨ç¯å½¢å›¾æ›¿ä»£é¥¼å›¾ï¼Œå¹¶ç§»é™¤å†…éƒ¨æ ‡ç­¾
        wedges, texts = ax.pie(
            values, 
            autopct=None,  # ç§»é™¤å†…éƒ¨ç™¾åˆ†æ¯”æ ‡ç­¾
            explode=explode, 
            startangle=90, 
            colors=pie_colors,
            wedgeprops={'width': 0.5, 'edgecolor': '#1E1E1E', 'linewidth': 0.5}
        )
        
        # æ·»åŠ å›¾ä¾‹åˆ°å·¦ä¾§
        ax.legend(
            wedges, 
            [f"{name} ({self.format_params(val)}, {val/sum(values)*100:.1f}%)" for name, val in zip(names, values)],
            loc="center right", 
            bbox_to_anchor=(0, 0.5),  # å›¾ä¾‹ä½ç½®æ”¹åˆ°å·¦ä¾§
            fontsize=9,
            framealpha=0.5,
            labelcolor='white'
        )
        
        ax.set_title("ç»„ä»¶å‚æ•°åˆ†å¸ƒ", color='white', fontsize=14)

    def draw_unet_structure(self, ax, data):
        """ä¿®æ”¹åçš„UNETç»“æ„å›¾ä½¿ç”¨Bå•ä½åæ ‡ç³»"""
        unet_parts = data.get('unet_parts', {})
        if not unet_parts:
            ax.text(0.5, 0.5, 'æ— UNETç»“æ„æ•°æ®', ha='center', va='center', color='white')
            return
        
        # è®¡ç®—ç»å¯¹å‚æ•°é‡
        total_unet_params = data['components'].get('UNET', 0)
        abs_parts = {}
        for part, ratio in unet_parts.items():
            abs_parts[part] = ratio * total_unet_params
        
        # æ’åºå¹¶åˆ›å»ºæ ‡ç­¾
        labels = []
        values = []
        for part, params in sorted(abs_parts.items(), key=lambda x: -x[1]):
            labels.append(part)
            values.append(params)
        
        # åˆ›å»ºæ°´å¹³æ¡å½¢å›¾
        bars = ax.barh(labels, values, color=['#2E5D87', '#3B8EA5', '#4BB3D3'])
        
        # æ·»åŠ å‚æ•°é‡æ ‡ç­¾ - ä½¿ç”¨åŠ¨æ€å•ä½ï¼ˆä¿æŒä¸å˜ï¼‰
        ax.bar_label(bars, [self.format_params(v) for v in values], 
                    padding=3, color='white', fontsize=10)
        
        # è®¾ç½®Xè½´ä¸ºå‚æ•°é‡ï¼ˆä½¿ç”¨0.5Bä¸ºæœ€å°é—´éš”å•ä½ï¼‰
        max_value = max(values) * 1.1
        max_value_in_b = max_value / 1e9
        
        # ç¡®å®šæœ€å¤§åˆ»åº¦å€¼ï¼ˆå‘ä¸Šå–æ•´åˆ°0.5Bçš„å€æ•°ï¼‰
        max_tick_value_in_b = math.ceil(max_value_in_b * 2) / 2
        
        # ç”Ÿæˆ0.5Bé—´éš”çš„åˆ»åº¦
        tick_values_in_b = [i * 0.5 for i in range(int(max_tick_value_in_b * 2) + 1)]
        x_ticks = [v * 1e9 for v in tick_values_in_b]
        
        ax.set_xticks(x_ticks)
        
        # ä½¿ç”¨Bå•ä½åæ ‡è½´åˆ»åº¦(0.5B, 1B, 1.5Bç­‰)
        x_tick_labels = []
        for value_in_b in tick_values_in_b:
            if value_in_b == int(value_in_b):
                # æ•´æ•°Bå€¼
                x_tick_labels.append(f"{int(value_in_b)}B")
            else:
                # 0.5Bçš„å€æ•°
                x_tick_labels.append(f"{value_in_b:.1f}B")
        
        ax.set_xticklabels(x_tick_labels, color='white')
        ax.set_xlim(0, max_value)
        
        ax.set_title("UNETå†…éƒ¨ç»“æ„", color='white', fontsize=14)

    def draw_param_bars(self, ax, data, arch_type):
        """ä¿®æ”¹åçš„æŸ±çŠ¶å›¾ä½¿ç”¨æ•´æ•°å•ä½"""
        # åŸºç¡€ç»„ä»¶ + æ›´å¤šå¯èƒ½çš„ç»„ä»¶
        components = ['UNET', 'VAE', 'CLIP']
        
        # å¦‚æœå­˜åœ¨EMAï¼Œæ·»åŠ åˆ°ç»„ä»¶åˆ—è¡¨
        if data['components'].get('EMA', 0) > 0:
            components.append('EMA')
        
        # æ·»åŠ æœªçŸ¥åˆ†ç±»
        if data['components'].get('æœªçŸ¥', 0) > 0:
            components.append('æœªçŸ¥')
        
        # æ·»åŠ å…¶ä»–åˆ†ç±»
        if data['components'].get('å…¶ä»–', 0) > 0:
            components.append('å…¶ä»–')
        
        params = [data['components'].get(c, 0) for c in components]
        
        # è¿‡æ»¤æ‰æ•°å€¼ä¸º0çš„ç»„ä»¶
        valid_indices = [i for i, p in enumerate(params) if p > 0]
        valid_components = [components[i] for i in valid_indices]
        valid_params = [params[i] for i in valid_indices]
        
        # é¢œè‰²æ˜ å°„
        colors = {
            'UNET': '#4C72B0', 
            'VAE': '#55A868', 
            'CLIP': '#C44E52', 
            'EMA': '#DD8452',
            'æœªçŸ¥': '#937860',
            'å…¶ä»–': '#8172B0'
        }
        valid_colors = [colors.get(comp, '#cccccc') for comp in valid_components]
        
        # åˆ›å»ºç®€æ´æ ‡ç­¾
        bars = ax.bar(valid_components, valid_params, color=valid_colors)
        
        # æ·»åŠ å‚æ•°æ•°å€¼æ ‡ç­¾ - ä½¿ç”¨åŠ¨æ€å•ä½ï¼ˆä¿æŒä¸å˜ï¼‰
        ax.bar_label(bars, [self.format_params(p) for p in valid_params], 
                    color='white', fontsize=10)
        
        # è®¾ç½®Yè½´ä¸ºå‚æ•°é‡ï¼ˆä½¿ç”¨0.5Bä¸ºæœ€å°é—´éš”å•ä½ï¼‰
        max_value = max(valid_params) * 1.1 if valid_params else 1
        max_value_in_b = max_value / 1e9
        
        # ç¡®å®šæœ€å¤§åˆ»åº¦å€¼ï¼ˆå‘ä¸Šå–æ•´åˆ°0.5Bçš„å€æ•°ï¼‰
        max_tick_value_in_b = math.ceil(max_value_in_b * 2) / 2
        
        # ç”Ÿæˆ0.5Bé—´éš”çš„åˆ»åº¦
        tick_values_in_b = [i * 0.5 for i in range(int(max_tick_value_in_b * 2) + 1)]
        y_ticks = [v * 1e9 for v in tick_values_in_b]
        
        ax.set_yticks(y_ticks)
        
        # ä½¿ç”¨Bå•ä½åæ ‡è½´åˆ»åº¦(0.5B, 1B, 1.5Bç­‰)
        y_tick_labels = []
        for value_in_b in tick_values_in_b:
            if value_in_b == int(value_in_b):
                # æ•´æ•°Bå€¼
                y_tick_labels.append(f"{int(value_in_b)}B")
            else:
                # 0.5Bçš„å€æ•°
                y_tick_labels.append(f"{value_in_b:.1f}B")
        
        ax.set_yticklabels(y_tick_labels, color='white')
        ax.set_ylim(0, max_value)
        
        # ç¾åŒ–å›¾è¡¨
        ax.set_ylabel("å‚æ•°é‡", color='white')
        ax.set_title(f"{arch_type} æ¨¡å‹ç»„ä»¶å‚æ•°åˆ†å¸ƒ", color='white', fontsize=16)
        ax.tick_params(axis='x', labelcolor='white', labelrotation=0)
        ax.grid(True, axis='y', alpha=0.3)

    def convert_image(self, base64_img):
        """å¢å¼ºå›¾åƒè½¬æ¢å¯é æ€§"""
        try:
            if not base64_img:
                return torch.zeros((1, 1, 3))
            # è§£æbase64æ•°æ®
            if ',' in base64_img:
                header, data = base64_img.split(",", 1)
            else:
                data = base64_img
            img_data = base64.b64decode(data)
            img = Image.open(io.BytesIO(img_data))
            # ä¿æŒåŸå§‹æ¯”ä¾‹è°ƒæ•´å°ºå¯¸
            max_size = 2048
            ratio = max_size / max(img.size)
            new_size = (int(img.size[0]*ratio), int(img.size[1]*ratio))
            img = img.resize(new_size, Image.Resampling.LANCZOS)
            img_tensor = torch.from_numpy(np.array(img)).float() / 255.0
            return img_tensor.unsqueeze(0)
        except Exception as e:
            logger.error(f"å›¾åƒè½¬æ¢å¤±è´¥: {str(e)}")
            return torch.zeros((1, 512, 512, 3))

    def update_progress(self, progress, desc):
        """æ›´æ–°è¿›åº¦æç¤º"""
        if hasattr(self, "progress_bar"):
            self.progress_bar.update(progress, desc)
        else:
            logger.info(f"[è¿›åº¦ {progress}%] {desc}")

    def generate_text_report(self, analysis_result):
        """ç”Ÿæˆæ–‡æœ¬æ ¼å¼çš„è¯¦ç»†åˆ†ææŠ¥å‘Š"""
        report = []
        # åŸºç¡€ä¿¡æ¯
        report.append(f"ğŸ“„ æ¨¡å‹æ–‡ä»¶: {analysis_result['model_name']}")
        report.append(f"ğŸ“Š æ¨¡å‹å¤§å°: {analysis_result['model_size']}")
        report.append(f"ğŸ”¢ å‚æ•°æ€»é‡: {analysis_result['total_params']}")
        
        # æ¶æ„ä¿¡æ¯
        report.append(f"\nğŸ—ï¸ æ¨¡å‹æ¶æ„: {analysis_result['architecture']['type']}")
        for info in analysis_result['architecture'].get('extra_info', []):
            report.append(f"  â€¢ {info}")
        
        # ç»„ä»¶åˆ†æ
        for comp in ['UNET', 'VAE', 'CLIP']:
            if comp in analysis_result['components']:
                report.append(f"\n{comp}ç»“æ„:")
                report.append(f"- å‚æ•°é‡: {analysis_result['components'][comp]['params']}")
                report.append(f"- å æ¯”: {analysis_result['components'][comp]['percentage']}%")
        
        # ç²¾åº¦åˆ†æ
        report.append("\nğŸ”§ ç²¾åº¦åˆ†æ:")
        report.append(f"- ä¸»è¦ç²¾åº¦: {analysis_result['precision']['primary']}")
        for dist in analysis_result['precision']['distribution']:
            report.append(f"- {dist}: {analysis_result['precision']['distribution'][dist]}%")
        
        # æ·»åŠ EMAå‚æ•°ç»Ÿè®¡åˆ°æ–‡æœ¬æŠ¥å‘Š
        if ema_params > 0:
            report.append(f"\nâš¡ EMAå‚æ•°:")
            report.append(f"- å‚æ•°é‡: {self.format_params(ema_params)} ({ema_params/total_params*100:.1f}%)")
            report.append(f"- é”®æ•°é‡: {len(ema_keys)}")
        
        return "\n".join(report) 